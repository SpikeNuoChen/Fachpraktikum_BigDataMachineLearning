{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klassifikation und Regression mit der *scikit-learn* Bibliothek\n",
    "\n",
    "## Klassifikation\n",
    "\n",
    "Bei einer Klassifikation wird ein Datenpunkt $\\mathbf{x}$ einer von $N$ Klassen zugeordnet. Die einzelnen Einträge von $\\mathbf{x}$ werden *Features* genannt, $\\mathbf{x}$ ist ein *Feature-Vektor*.\n",
    "<br>\n",
    "**Zwei Klassen und zwei Features**:\n",
    "<br>\n",
    "<img src=\"2_classes.png\" alt=\"2 Klassen\" width=\"300\"/>\n",
    "<br>\n",
    "**Drei Klassen und drei Features**:\n",
    "<br>\n",
    "<img src=\"3_classes_3d.png\" alt=\"3 Klassen\" width=\"300\"/>\n",
    "\n",
    "Ein Klassifikator ist eine *Funktion*, die einen Datenpunkt erhält und für diesen eine Klasse ausgibt: \n",
    "$$ f(\\mathbf{x}) = \\text{class}$$\n",
    "Ziel ist es nun eine *Funktion* $f$ zu finden, die für möglichst alle Datenpunkte die richtige Klasse ausgibt. Dafür gibt es bereits verschiedene Machine Learning Verfahren. Die gängigsten sind in der Python-Bibliothek *scikit-learn* implementiert.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übungsaufgaben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: Iris\n",
    "\n",
    "Verfügbar unter: https://datahub.io/machine-learning/iris/r/iris.csv. Dieser Datensatz enthält 4 Features für 3 verschiedene Spezies (Klassen) der Schwertlilie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting library\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# render plots in jupyter notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Datensatz einlesen und vorverarbeiten\n",
    "\n",
    "a) Lies den Datensatz mithilfe von Pandas ein und mach dich mit ihm vertraut. Wie groß ist der Datensatz? Welche Features gibt es?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Unterteile das DataFrame in die zwei DataFrames ```df_features``` und ```df_class```, die nur die entsprechenden Größen enthalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) ```df_class``` enthält die Klassen in Form von Strings. Diese Darstellung ist für die nachfolgenden Arbeitsschritte ungeeignet. Bringe das DataFrame ```df_class``` mithilfe von Pandas in folgende Form:<br>\n",
    "<img src=\"one_hot_encoded.png\" alt=\"One hot encoded\" width=\"200\"/>\n",
    "\n",
    "**Tipp:** Dieses Darstellung nennt man auch *One Hot Encoding*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Um zu überprüfen, ob der Klassifikator fremde Datenpunkte zuordnen kann, muss der Datensatz in ein *Trainingsset* und ein *Testset* unterteilt werden. Der Klassifikator wird auf dem *Trainingsset* trainiert und seine Performace auf dem *Testset* validiert, welches er zuvor nie gesehen hat. So wird sichergestellt, dass er die Datenpunkte nicht \"auswendig lernt\". \n",
    "\n",
    "Teile den Datensatz mithilfe der *scikit-learn*-Bibliothek in *Trainings-* und *Testset* auf. Dabei soll der Trainingsdatensatz 75% der Datenpunkte enthalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. k-nearest-neighbors\n",
    "\n",
    "Der *k-nearest-neighbors*-Klassifikator vergleicht einen fremden Datenpunkt mit den $k$ Datenpunkten aus dem *Trainingsset* , welche die kürzeste euklidsche Distanz zu dem neuen Datenpunkt besitzen. Der Datenpunkt erhält dann die Klasse, die der Mehrheit der umliegenden Datenpunkte entspricht. Darum wird $k$ immer ungerade gewählt.\n",
    "\n",
    "a) Importiere den *k-nearest-neighbors*-Klassifikator aus der *scikit-learn*-Bibliothek und initialisiere eine Instanz des Klassifikators mit 7 nächsten Nachbarn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Trainiere den Klassifikator auf dem Trainingsdatensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Der Klassifikator kann jetzt einen neuen Datenpunkt erhalten und ihn einer der drei Klassen zuordnen. Wende die ```predict()```-Methode des Klassifikators auf die Test-Datenpunkte an und lass dir das Ergebnis ausgeben.  Was bedeutet die Ausgabe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<img src=\"table.jpeg\" alt=\"Tabelle\" width=\"600\"/>\n",
    "<br>\n",
    "\n",
    "d) \n",
    "\n",
    "\n",
    "Die Accuracy des Klassifikators kann berechnet werden als \n",
    "$$ \\text{Accuracy} = \\frac{\\text{#TP + #TN}}{\\text{#TP + #TN + #FP + #FN}}$$\n",
    "mit TP = True Positives, TN = True Negatives, FP = False Positives und FN = False Negatives.\n",
    "Berechne die Accuracy auf dem Testdatensatz und überprüfe dein Ergebnis mit der ```score()```-Methode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Verändere die Anzahl an nächsten Nachbarn im Klassifikator und beobachte, wie sich dieser Parameter auf die Accuracy auswirkt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decision Trees\n",
    "\n",
    "Decision Trees klassifizieren den Datensatz anhand einfacher Entscheidungsregeln, z.B:\n",
    "```python\n",
    "if Feature_1 > 0.5:\n",
    "    return Class_A\n",
    "else:\n",
    "    return Class_B\n",
    "```\n",
    "Um die Koplexität des Datensatzes widerzuspiegeln werden mehrere dieser Entscheidungsknoten aneinandergehängt. Diese Knoten bilden dann den Entscheidungsbaum. Der Decision Tree Algorithmus versucht Entscheidungsregeln zu finden, die den Trainingsdatensatz bestmöglich trennen.\n",
    "\n",
    "a) Importiere den *DecisionTree*-Klassifikator aud der *scikit-learn*-Bibliothek und initialisiere eine Instanz des Klassifikators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Nimm eine Klassifikation auf dem Iris-Datensatz vor. Benutze dieses mal nur die Features 'sepallength' und 'sepalwidth' für die Klassifikation. Berechne die Accuracy mit mit der ```score()```-Methode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Führe den nachfolgenden Code aus. Was genau wird abgebildet? In welchen Merkmalen des Schaubildes spiegelt sich die Funktionsweise eines Entscheidungsbaumes wider?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "colors = [(0, 0, 1), (1, 0.6, 0), (0, 1, 0)]\n",
    "cm = LinearSegmentedColormap.from_list(\n",
    "        'custom', colors, N=3)\n",
    "\n",
    "x_min, x_max = X_train['sepallength'].min() - 1, X_train['sepallength'].max() + 1\n",
    "y_min, y_max = X_train['sepalwidth'].min() - 1, X_train['sepalwidth'].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "\n",
    "Z = np.argmax(clf.predict(np.c_[xx.ravel(), yy.ravel()]), axis=1)\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.5, cmap=cm)\n",
    "\n",
    "sns.scatterplot(x='sepallength', y='sepalwidth', data=df, hue='class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random Forest und Metriken\n",
    "\n",
    "Ein Random Forest Klassifikator setzt sich aus mehreren Decision Trees zusammen. Beim Prädizieren wird die finale Klasse durch eine Mehrheitsentscheidung festgelegt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Lies den Datensatz *3d_printer.csv* mit pandas ein und teile den Datensatz in Testset und Trainingsset auf. Das Label/ die Zielgröße soll dabei die Spalte *material* sein. Transfomiere dieses mal die kategorischen Spalten *infill_pattern* und *material* mit dem ```LabelBinarizer``` von *scikit-learn* und dessen Methode ```fit_transform()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Importiere den Random Forest Klassifikator der *scikit-learn*-Bibliothek mit ```n_estimators=50``` und ```max_depth=3``` und trainiere diesen auf dem Datensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Accuracy eine Klassifikators ist nur eine vielen Metriken, mit denen sich die Qualität eines Models messen lässt. Für Datensätze mit einer sehr ungleichen Klassenverteilung kann die Accuracy hoch sein, da der Klassifikator meist die häufigere Klasse prädiziert. Aus der Accuracy kann dann jedoch keine Aussage über die Fähigkeiten des Modells bezüglich der kleineren Klasse getroffen werden. Für diesen Fall wählt man häufig die Metriken Precision und Recall. Die Precision ist definiert als $$ \\text{Precision} = \\frac{\\text{#TP}}{\\text{#TP + #FP}}$$ Der Recall ist gegeben durch $$ \\text{Recall} = \\frac{\\text{#TP}}{\\text{#TP + #FN}}$$\n",
    "\n",
    "c) Berechne Precision und Recall für obigen Klassifikator mithilfe der eingebauten scikit Funktionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Support Vector Machine\n",
    "\n",
    "Die Support Vector Machine versucht eine Hyperebene zu finden, die einen gegebenen Datensatz bestmöglich trennt:\n",
    "<br>\n",
    "<img src=\"2_classes_svm.png\" alt=\"2 Klassen\" width=\"300\"/>\n",
    "\n",
    "Diese Hyperebene ist definiert über einen Normalenvektor $\\mathbf{w}$, der durch den Ursprung verläuft und einen Abstand zum Ursprung $b$. Ziel ist es also $\\mathbf{w}$ und $b$ so zu wählen, dass der Datensatz möglichst eindeutig getrennt werden kann. Punkte auf der Hyperebene erfüllen die Ebenengleichung\n",
    "$$ \\mathbf{w} \\cdot \\mathbf{x} + b = 0.$$\n",
    "Für Punkte links oder rechts der Hyperebene gilt\n",
    "$$ \\mathbf{w} \\cdot \\mathbf{x} + b < 0 \\quad \\text{bzw.} \\quad \\mathbf{w} \\cdot \\mathbf{x} + b > 0 .$$\n",
    "Für zwei Klassen kann dann eine Klassifikation mittels $y_i = \\text{sgn}(\\mathbf{w} \\cdot \\mathbf{x}_i + b)$ erfolgen. Der Abstand zwischen Datenpunkten und Hyperebene ist dann maximal, wenn der Betrag von $\\mathbf{w}$ minimal ist unter der Nebenbedingung, dass sich möglichst alle Datenpunkte auf der korrekten Seite der Hyperebene befinden, geschrieben als $y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1$. Dies führt zu dem Optimierungsproblem\n",
    "$$ L(\\mathbf{w}, b, \\lambda) = \\frac{1}{2} ||\\mathbf{w}||^2 - \\sum_{i=1}^N \\lambda_i (y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b)-1) $$\n",
    "\n",
    "Im Realfall lässt sich ein Datensatz meist nicht perfekt sauber trennen. Um das Minimierungsproblem trotzdem lösen zu können wird daher noch ein Term mit Schlupfvariablen $\\xi_i$ eingeführt, der die strenge Minimierung auflockert und mit einer positiven Konstante $C$ skaliert wird:\n",
    "$$ C \\sum_{i=1}^N\\xi_i$$\n",
    "\n",
    "a) Im unteren Block wird ein synthetischer Datensatz aus einer Sinus-Funktion und Rauschen konstruiert. Plotte den Datensatz mithilfe von *matplotlib* oder *seaborn*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(0,4,100).reshape(-1, 1)\n",
    "y = np.sin(X)+ np.random.uniform(low=-0.3, high=0.3, size=X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Importiere den Support Vector Regressor der *scikit-learn*-Bibliothek und initialiere eine Instanz dieser Klasse. Nimm wieder einen Train-Test-Split vor und trainiere den Regressor auf den Trainingsdaten. Prädiziere nun auf den Testdatensatz und stelle die vorhergesagten Werte sowie den ursprünglichen Datensatz in einem Diagramm dar. Wähle für die Support Vector Machine die Option ```C``` manuell. Wähle dabei für ```C``` die Werte 0.01, 10 und 1000. Was funktioniert am besten? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Wir wollen nun versuchen, den $y$-Wert zu einem einzelnen Datenpunkt vorherzusagen. Prädiziere mit deinem Regressor den $y$-Wert zu einem Wert von $x>4$ und vergleiche es mit $\\sin (x)$. Was fällt dir auf? Wie kommt der Unterschied zustande? Tipp: Die ```predict()```-Methode benötigt einen Wert in der Form ``` predict([[5]])```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Klassifikation mit SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In den folgenden Blöcken ist das Training einer Support Vector Machine zur Klassifikation des Moon-Datensatzes implementiert. Führe die Codeblöcke zunächst aus und versuche alle Trainingsschritte nachzuvollziehen. Erhöhe jetzt die Anzahl der Trainingspunkte (bis maximal 10000). Was fällt dir beim Training auf? Miss die Dauer des Trainings mithilfe der Python-Module `time` oder `datetime` und plotte die Trainingsdauer in Abhängigkeit der Anzahl an Datenpunkten. Welches Verhalten kannst du erkennen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "noisy_moons, labels = datasets.make_moons(n_samples=200, noise=.2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(noisy_moons, labels, test_size=0.25)\n",
    "\n",
    "\n",
    "clf = SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "\n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# title for the plots\n",
    "title = ('Decision surface of linear SVC ')\n",
    "# Set-up grid for plotting.\n",
    "X0, X1 = noisy_moons[:, 0], noisy_moons[:, 1]\n",
    "xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "ax.scatter(X0, X1, c=labels, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "ax.set_ylabel('y label here')\n",
    "ax.set_xlabel('x label here')\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())\n",
    "ax.set_title(title)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
